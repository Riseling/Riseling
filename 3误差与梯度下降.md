# 三、偏差与梯度下降

## Bias and Variance

一个模型的误差主要来源于两部分：bias and variance

* **bias**: 估计值与真实值之间存在的差异
* **Variance**: 描述数据之间的离散程度，即波动范围

在实际情况中，我们是通过样本来进行推断，所以得到的结果都是估计值，一般会使用$\hat y$的方式来表示。关于bias和variance见下面例子

设变量$X$的观测值为$x_1,x_2,\cdots,x_n$, 且真实的均值（期望）为$\mu$，方差为$\sigma^2$，但是都未知

现通过观测值来估计均值和方差
$$
\hat \mu = \frac{1}{n}\sum_{i=1}^n x_i \neq \mu \\
{\rm Var}(\hat \mu)=\frac{\sigma^2}{n}
$$
可以看出$\hat \mu$会根据样本的不同而变化，必定会与真实存在差异，从而导致bias；而其方差则与$n$有关，$n$越大，方差越小，结果就越精确
$$
s^2=\frac{1}{n}\sum_{i=1}^n(x_i-\hat \mu)^2\\
E[s^2]=\frac{n-1}{n}\sigma^2 \neq \sigma^2 
$$
$s^2$的期望并不等于真实的总体方差，我们称这种估计叫做有偏估计(Biased estimator)，相应的，也存在无偏估计(Unbiased estimator)

下图为bias和variance的形象展示

![image-20210917151445383](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917151445383.png)

越简单的模型，受样本数据的影响越小，方差就越小

#### Solve large bias

* 输入更多features
* 使用更复杂的模型

#### Solve large variance

* 使用更多数据，但不一定总是起作用
* 正则化，通过正则化，可以是拟合更加平滑，避免过大的波动

### Model Selection

通常情况下，Bias越大，Variance越小(underfitting)；Bias越小，Variance越大(overfitting)，所以需要根据实际情况选择合适的模型，将Bias和Variance控制在合理范围之内

以下是几种常用的方法

#### Cross Validation(交叉验证)

![image-20210917153709320](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917153709320.png)

将训练集分成测试集和验证集，当两部分的误差都较小，则可以选择该模型

#### N-fold Cross Validation

![image-20210917153827844](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917153827844.png)

将数据集随机平均分成$N$份，每次将其中$1$份作为验证集，另外的$N-1$份作为训练集，一共进行$N$次，计算这$N$次验证集误差的平均值，平均误差越小，则模型效果越好

## Gradient Descent

回顾上一章内容，梯度下降法是解决如下优化问题
$$
\theta^*={\rm arg \ min }L(\theta)
$$
其中$L$为loss function, $\theta$为求解参数

解法如下

![image-20210917155308534](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917155308534.png)

其中$\eta$称为learning rate, 是人为调整的

<img src="C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917155912675.png" alt="image-20210917155912675" style="zoom:50%;" />

即为Gradient，是Loss的等高线的法线方向，如图示

![image-20210917160044197](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917160044197.png)

直到结果参数变化很小，即收敛到一定范围内停止，接下来再对梯度下降进行更详细的讲解

### Tip 1: Tuning your learning rates

$\eta$的设置必须仔细考虑，这会决定下降的速率，会影响计算机计算的时间以及结果正确性。设置过大，可能会错过局部最优值，设置过小，又可能计算时间过长

![image-20210917160512052](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917160512052.png)

如图中黄线为$\eta$设置过大，错过了局部最优值；绿线同理，且导致了无法求解；蓝线设置过小，计算量较大

注意：左图只有1维和2维情况才能可视化，所以一般采用右图的方式

#### Adaptive Learning Rates

* 普遍且简单的想法: 在每一步通过某些因素减小learning rate
  * 开始时，我们离目标值较远，可以使用较大的learning rate
  * 在迭代几次后，离目标渐近，所以减小learning rate
  * E.g. $1/t$ decay: $𝜂^𝑡 = 𝜂/\sqrt{𝑡 + 1}$
* Learning rate 不可能始终如一
  * 不同的parameters使用不同的learning rates  

##### Adagrad

关健：不同的参数$\eta$会不同

##### ![image-20210917163656574](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917163656574.png)

* $\sigma$是前t次对$w$求微分的平方和的均值开根号，所以不同参数的$\eta$会不同

![image-20210917163712195](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917163712195.png)

* Divide the learning rate of each parameter by the root mean square of its previous derivatives  

![image-20210917163758813](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917163758813.png)

#### Contradiction

![image-20210917184857147](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917184857147.png)

* 直观解释：分母这一项，就是用来评估这个反差的效果

![image-20210917185318538](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917185318538.png)

* 1个参数时，梯度越大，离最低点距离越远，下图以二次函数为例

![image-20210917185731762](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917185731762.png)

* 多个参数时，上述结论不成立

![image-20210917185852668](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917185852668.png)

* best step：承接二次函数的例子，距离不仅与一次微分呈正比，还与二次微分呈反比。这样才能真正显示最好的距离

![image-20210917190020793](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917190020793.png)

* 因此，Adagrad就类似于上述关系，分母其实就是估计二次微分

![image-20210917190615104](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917190615104.png)

### Tip 2: Stochastic Gradient Descent

原理：每次随机取1个样本$x_n$，计算其Loss，记为$L^n$，然后就更新一次参数

![image-20210917190921334](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917190921334.png)

* 左图为一般方法，路径比较稳定
* 右图则为随机方法，更新会更快

### Tip 3: Feature Scaling

假设做如下回归

![image-20210917164626363](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917164626363.png)

当feature的度量不一致时，不同feature数据差异是不一致的，若直接使用梯度下降，会导致如左图的结果——只在几乎一个方向上下降，若标准化后，则如右图，更易找到合适的下降

#### 处理方法

![image-20210917164852495](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917164852495.png)

这样，标准化后，各个feature的均值为0，方差都为1，度量则统一了

### 理论推导

* **NOTE**

**Question**：每次更新是Loss更小，则这个参数正确吗？

**Answer**：不一定，本就可能下降

* **Formal** **Derivation**

![image-20210917191632681](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917191632681.png)

现在问题是，如何在指定点附近，寻找到最小值？

* 前提：**Taylor Series**

可以将任意函数进行泰勒展开，如$h(x)$可以写成

![image-20210917191742647](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917191742647.png)

多元函数同样可以泰勒展开，如$h(x,y)$可以写成

![image-20210917191903385](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917191903385.png)

* 设Loss function为$L(\theta)$

![image-20210917192217570](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917192217570.png)

![image-20210917192245012](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917192245012.png)

![image-20210917192423426](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917192423426.png)

对于上图2元的情况，实际上就是在定义圆内找到使$L(\theta)$最小的点，且$L(\theta)$是一条直线，只要使得$\Delta \theta_1$与$\Delta \theta_2$，与直线系数$u,v$相反，且找到最远的点即可

红色圆圈半径越小，式子越成立，所以需要learning rate足够小，但实际上也不能太小

如果考虑到二次式（牛顿法），则learning rate可稍微大些，但是会明显增加运算量

### 局限

梯度下降可能会在如下位置停止

* 局部最小值的位置
* 微分为或近似0的地方

![image-20210917193422778](C:\Users\luozl\AppData\Roaming\Typora\typora-user-images\image-20210917193422778.png)

